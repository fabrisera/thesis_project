\chapter{Methods}

\section{Mixed Effects Models}

The main statistical method used in this study is the linear mixed-effects model (LMM) implemented via the R package lme4 \cite{lmer}. LMMs are a generalization of linear regression models that allow for the inclusion of both fixed and random effects. Fixed effects are the parameters of interest, while random effects are used to account for the correlation between observations within the same group. Mixed-effects models are particularly useful for longitudinal data, where repeated measurements are taken on the same subjects over time. In our case, the repeadted measurements are the yearly company CDP reports, given that the same companies report their emissions over multiple years. As Bates explains, mixed-effects models are also known as \textit{multilevel} models becauase the random effects represent levels of variation in addition to the pre-observation noise term that is incorporated in common statistical models such as linear regression models, generalized linear models, and nonlinear regression models \cite{bates}. 

\subsection{Model Specification}
Let's consider a simple linear mixed-effects model having next year decarbonization rate as a response with an intercept, a single fixed effect, and a single random intercept. In our case, the fixed effect will be the year of the report, and the random effect will be the company. Let $y_{ij}$ be the response variable at the $i$-th year for the $j$-th company, let $x_{ij}$ be the fixed effect at the $i$-th year for the $j$-th company, and let $z_{ij}$ be the random effect at the $i$-th year for the $j$-th company. The linear mixed-effects model can be written as \cite{duke_sta216_lecture}:
\begin{align}
    Y_{ij} &= \alpha_j + \beta_0 + \beta_1 X_{ij} + \epsilon_{ij} \\
    \epsilon_{ij} &\sim N(0, \sigma^2) \\
    \alpha_j &\sim N(\mu_{\alpha}, \sigma^2_{\alpha}) \\
    i &= 1, 2, ..., n_j \\
    j &= 1, 2, ..., J
\end{align}
Where $J$ is the number of companies, $n_j$ is the number of years for the $j$-th company, $\alpha_j$ is the random intercept for the $j$-th company, $\mu_{\alpha}$ is the mean of the random intercepts, $\sigma^2_{\alpha}$ is the variance of the random intercepts, $\beta_0$ is the fixed intercept, $\beta_1$ is the fixed effect, and $\epsilon_{ij}$ is the error term. Note how the corresponding OLS model would require to control for the company fixed effects by including a dummy variable $D_j$ for each company:
\begin{align}
    Y_{ij} &= \beta_0 + \beta_1 X_{ij} + \sum_{j=1}^{J} \alpha_j D_j + \epsilon_{ij}
\end{align}
In the ordinary least squares (OLS) model, we introduce a separate parameter for each of the $J$ groups which leads to the estimation of $J$ additional coefficients. This can be inefficient, particularly when $J$ is large. In contrast, in the mixed-effects model we introduce only two additional parameters for the random effects: $\sigma^2_{\alpha}$, which represents the variance of the random intercepts across groups, and $\sigma^2_e$, which represents the residual variance within groups. These parameters are estimated from the data using the Restricted Maximum Likelihood (REML) method \cite{bates}. REML is particularly effective because it corrects for the potential bias introduced by estimating fixed effects alongside the variance components. This approach enhances model efficiency and reduces the risk of overfitting, especially in scenarios with a large number of groups. Once the variance components $\sigma^2_{\alpha}$ and $\sigma^2_e$ are estimated, the model employs a process known as \textit{shrinkage} to compute the individual random effects (intercepts) for each group. Shrinkage is a regularization technique that adjusts the individual group estimates, pulling them towards a central value, typically towards the overall population mean, which in the context of many mixed models is assumed to be zero \cite{bates}. This shrinkage effect is governed by the relative sizes of $\sigma^2_{\alpha}$ and $\sigma^2_e$. Specifically:
\begin{itemize}
    \item If $\sigma^2_{\alpha}$ is large compared to $\sigma^2_e$, it indicates significant variability among the groups. Thus, the random intercepts for each group are allowed to deviate more from the overall mean, reflecting the distinct characteristics of each group.
    \item Conversely, if $\sigma^2_{\alpha}$ is small relative to $\sigma^2_e$, it suggests that the groups are not markedly different from each other. As a result, the random intercepts are more heavily shrunk towards the central value, reducing the differences among group intercepts.
\end{itemize}
This approach allows the mixed-effects model to balance between capturing the unique attributes of each group and maintaining model parsimony and generalizability, making it particularly useful when dealing with a large number of groups.

\subsection{Extension to Multiple Fixed Effects and Random Slopes}
The linear mixed-effects model can be extended to include multiple fixed effects and random slopes. Let $X$ be a $n \times p$ matrix of fixed effects, where $n$ is the number of observations and $p$ is the number of fixed effects. Let $Z$ be a $n \times q$ matrix of random effects, where $q$ is the number of random effects. The linear mixed-effects model can be written as \cite{duke_sta216_lecture}:
\begin{align}
    Y &= X\beta + Z\gamma + \epsilon \\
    \epsilon &\sim N(0, \sigma^2I) \\
    \gamma &\sim \mathcal{N}(\mathbf{0}, \sigma^2\Sigma) \\
    \gamma &\perp \epsilon
\end{align}
We followed the notation of Baayen et al \cite{BAAYEN2008390}, where $Y$ is a $n \times 1$ vector of response variables, $\beta$ is a $p \times 1$ vector of fixed effects, $\gamma$ is a $q \times 1$ vector of random effects, $\epsilon$ is a $n \times 1$ vector of error terms, $\Sigma$ is a $q \times q$ covariance matrix of random effects, and $I$ is the identity matrix. The covariance matrix $\Sigma$ is estimated from the data using the REML method in the lmer package \cite{bates}. 



% Using Kenward-Roger or Satterthwaite approximations for evaluating significance in linear mixed-effects models in R provides more conservative results than likelihood ratio tests and t-as-z, especially for smaller sample sizes \cite{Luke2016Evaluating}.

\section{Bayesian Ridge Regression}
We will now describe the Bayesian Ridge Regression model. Bayesian techniques can be used to include regularization parameters which is not set in a hard sense but tuned to the data. This can be done by indtroducing uninformative priors over the hyper parameters of the model \cite{scikit-learn, mackay_bayesian_1992}. The $l_2$ regularization parameter used in Ridge regression is equivalent to finding a maximum a posetiori estimation under a Gaussian prior over the coefficients $w$ with precision $\alpha^{-1}$ \cite{scikit-learn}. Effectively, we can use a Bayesian method to replicate regularization, with the advantage of being able to build confidence intervals for the coefficients. We will report the specification presented by Tipping \cite{tipping} which is implemented in the scikit-learn library \cite{scikit-learn}.

\subsection{Model Specification}
Consider a data set of input-target pairs $\{\mathbf{X_n}, t_n\}^N_{n = 1}$ we follow the standard probabilistic formulation and assume that targets are samples from the model with additive noise: 
\begin{align}
    t_n = y(\mathbf{x}_n, \mathbf{w}) + \epsilon_n
\end{align}
where $\epsilon_n$ iare independent samples from some noise process which is assumed to be mean-zero Gaussian with variance $\sigma^2$ \cite{tipping}. Therefore we have:
\begin{align}
    p(t_n|\mathbf{x}) = \mathcal{N}(t_n|y(\mathbf{x}_n), \sigma^2)
\end{align}
the notation specifies a Gaussian distribution over the target variable $t_n$ with mean $y(\mathbf{x}_n, \mathbf{w})$ and variance $\sigma^2$. Assuming independence of $t_n$, we can write the likelihood of the complete data set as \cite{tipping}:
\begin{align}
    p(\mathbf{t}| \mathbf{w}, \sigma^2) &= (2 \pi \sigma^2)^{-\frac{N}{2}} \exp\left\{-\frac{1}{2\sigma^2}||\mathbf{t} - \Phi \mathbf{w}||^2\right\} \\
    t &= (t_1, t_2, ..., t_N)^T \\
    w &= (w_1, w_2, ..., w_N)^T
\end{align}

\noindent Then, we encode a preference of smoother functions by using a zero-mean Gaussian prior over the parameters $\mathbf{w}$, with precision $\alpha$ \cite{tipping}:
\begin{align}
    p(\mathbf{w}|\alpha) &= \prod_{i = 0}^{N} \mathcal{N}(w_i|0, \alpha^{-1}) \\
\end{align}
where $\mathbf{\alpha}$ is a vector of $N + 1$ hyperparameters. Note that there is an individual hyperparameter associated independently with every weight. To complete the specification of this hierarchical model, we define hyperparameters over $\mathbf{\alpha}$ as well as the final remaining hyperparameter $\sigma^2$, we do so by using Gamma priors\cite{tipping}:
\begin{align}
    p(\alpha_i) &= \text{Gam}(\alpha_i|a, b) \\
    p(\beta) &= \text{Gam}(\beta|c, d) \\
    \beta \equiv \frac{1}{\sigma^2} \\
    \text{Gam}(\alpha|a, b) &= \Gamma(a)^{-1} b^a \alpha^{a - 1} \exp(-b\alpha)
\end{align}

\subsection{Non informative Priors}
The key aspect of the model is that we intentionally make these priors non-informative by fixing their parameters to small values e.g. $a = b = c = d = 10^{-4}$ \cite{tipping}. The incredibly interesting aspect of such approach is that this formulation of priors is a type of \textit{automatic relevance determination} \cite{MacKay1996} where a broad prior over the hyperparameters allows the posterior to concentrate at very large values of some of those $\alpha$ values, with the advantage that the associated posterior weights probability will be concentratd around zero, effectively eliminating the corresponding inputs and deeming them as irrelevant, thus providing a form of automatic feature selection \cite{tipping}. 


\section{CatBoost Regression}

CatBoost is a state-of-the-art machine learning algorithm developed by Yandex, specifically designed to efficiently handle categorical variables \cite{prokhorenkova2018catboost}.

\subsubsection{Handling Categorical Features}

CatBoost transforms categorical features into numerical values using an efficient strategy which reduces overfitting and allows to use the whole dataset for training \cite{dorogush2018catboost}. In particular, a random permutation of the dataset is performed and for each example the algorithm computes the average label value for the example with the same category value placed before the given one in the permutation \cite{dorogush2018catboost}. Let $\sigma = (\sigma_1, \sigma_2, ..., \sigma_n)$ be the permutation of the dataset, then $x_{\sigma_p, k}$ is substituted with:
\begin{align}
    \frac{\sum_{i=1}^{p-1} [x_{\sigma_i, k} = x_{\sigma_p, k}] \cdot y_{\sigma_j} + a \cdot P }{\sum_{i=1}^{p-1} [x_{\sigma_i, k} = x_{\sigma_p, k}] + a}
\end{align}
\noindent finally, the algorithm uses the transformed variable to build the decision tree \cite{dorogush2018catboost}.

\subsubsection{Gradient Boosting Framework}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/gradient_boosting_v2.png}
    \caption{Gradient boosting. Source: \cite{Yenigun2022}}
    \label{fig:catboost}
\end{figure}

In this section I will briefly highlight Catboost's unique features. The CatBoost algorithm is based upon oblivious decision trees, which are trees that make decisions based on only one feature at a time \cite{dorogush2018catboost}, this design ensures that the threes are balanced and less prone to overfitting. At the same time, using oblivious trees allows for a more efficient implementation and faster execution speed. In Ordered boosting mode \cite{dorogush2018catboost}, Catboost usues supporting models $M_{r,j}$ to make predicitons based on peromutations of training samples. Note that the random permutation $\sigma_r$ chosen for each tree-building iteration are the same used to calculate the Target Statistics for categorical features embeddings. Finally, the implementation of Catboost I will be using in the following chapters uses \textit{l2} regularization on the leaf values of the trees to prevent overfitting \cite{dorogush2018catboost,prokhorenkova2018catboost}. For a general overview of gradient boosting, the literature is vast and I suggest to go over the Survey from He et al. \cite{he2019gradient} or alternatively the overview of XGBoost, a popular tree bosting system, by Chen et al. \cite{Chen_2016}. 




% \section{Bayesian Ridge Regression}

% \section{CatBoost Regression}

% \section{Neural Networks}
